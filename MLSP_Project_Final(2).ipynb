{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSP Project - Final(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtp8WaCUvOI0",
        "outputId": "cb7e063c-b235-4ae0-8681-86c878e69504"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ1K9D7yvQ8C",
        "outputId": "2ef68a12-c3b4-4c4e-f94c-4e2136e5a3a3"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import scipy.io as sio\r\n",
        "import librosa\r\n",
        "import pdb\r\n",
        "import string\r\n",
        "import os\r\n",
        "import random\r\n",
        "\r\n",
        "!pip install python-levenshtein\r\n",
        "\r\n",
        "from Levenshtein import distance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i48VzhrvUnY"
      },
      "source": [
        "def wav2feat(wavfile):\r\n",
        "    '''\r\n",
        "    Input: audio wav file name\r\n",
        "    Output: Magnitude spectrogram\r\n",
        "    '''\r\n",
        "    x, Fs = librosa.load(wavfile, sr=44100, mono=True) \r\n",
        "    hop = int(0.01 * Fs) # 10ms\r\n",
        "    win = int(0.02 * Fs) # 20ms\r\n",
        "    X = librosa.stft(x, n_fft=1024, hop_length=hop, win_length=win, window='hann', center=True, pad_mode='reflect')\r\n",
        "    return np.abs(X)\r\n",
        "\r\n",
        "def wavs2feat(wavfiles):\r\n",
        "    '''\r\n",
        "    Concatenate the audio files listed in wavfiles\r\n",
        "    Input: list of audio wav file names\r\n",
        "    Output: Magnitude spectrogram of concatenated wav\r\n",
        "    '''\r\n",
        "    x = []\r\n",
        "    for wf in wavfiles:\r\n",
        "        x1, Fs = librosa.load(wf, sr=44100, mono=True)\r\n",
        "        x.append(x1)\r\n",
        "    x = np.hstack(x)\r\n",
        "    hop = int(0.01 * Fs) # 10ms\r\n",
        "    win = int(0.02 * Fs) # 20ms\r\n",
        "    X = librosa.stft(x, n_fft=1024, hop_length=hop, win_length=win, window='hann', center=True, pad_mode='reflect')\r\n",
        "    return np.abs(X)\r\n",
        "\r\n",
        "def read_csv(filename):\r\n",
        "    id_label = {}\r\n",
        "    with open(filename,'r') as fid:\r\n",
        "        for line in fid: # '176787-5-0-27.wav,engine_idling\\n'\r\n",
        "            tokens = line.strip().split(',') # ['176787-5-0-27.wav', 'engine_idling']\r\n",
        "            id_label[tokens[0]] = tokens[1]\r\n",
        "    return id_label\r\n",
        "\r\n",
        "def editDistance(gt, est):\r\n",
        "    '''both are lists of labels\r\n",
        "    E.g. gt is \"dog_bark-street_music-engine_idling\"\r\n",
        "    E.g. est is \"street_music-engine_idling\"\r\n",
        "    '''\r\n",
        "    gttokens = gt.split('-')\r\n",
        "    esttokens = est.split('-')\r\n",
        "    # Map token to char\r\n",
        "    tokenset = list(set(gttokens+esttokens)) # ['dog_bark', 'siren', 'street_music', 'engine_idling']\r\n",
        "    token_char = {}\r\n",
        "    for i in range(len(tokenset)):\r\n",
        "        token_char[tokenset[i]] = string.ascii_uppercase[i]  # {'dog_bark': 'A', 'siren': 'B', 'street_music': 'C', 'engine_idling': 'D'}\r\n",
        "    # convert gt and est to strings\r\n",
        "    gtstr = [token_char[t] for t in gttokens]\r\n",
        "    gtstr = ''.join(gtstr)  # 'BCA'\r\n",
        "    eststr = [token_char[t] for t in esttokens]\r\n",
        "    eststr = ''.join(eststr)  # \r\n",
        "    # Compare\r\n",
        "    editdist = distance(gtstr, eststr) # 1\r\n",
        "    score = max(0, 1 - editdist/len(gtstr))\r\n",
        "    return editdist, score\r\n",
        "\r\n",
        "def evals(gtcsv, estcsv, taskid):\r\n",
        "    gt_id_label = read_csv(gtcsv)\r\n",
        "    est_id_label = read_csv(estcsv)\r\n",
        "    score = 0\r\n",
        "    for id in est_id_label:\r\n",
        "        if taskid==1:\r\n",
        "            if est_id_label[id] == gt_id_label[id]:\r\n",
        "                score += 1\r\n",
        "        elif taskid==2:\r\n",
        "            _, ss = editDistance(gt_id_label[id], est_id_label[id])\r\n",
        "            score += ss\r\n",
        "        else:\r\n",
        "            pdb.set_trace()\r\n",
        "            assert False, [\"taskid not correct; it is\", taskid]\r\n",
        "    avgScore = score/len(est_id_label)\r\n",
        "    return avgScore\r\n",
        "\r\n",
        "# if __name__==\"__main__\":\r\n",
        "    # wavs = ['../shared_train/audio_train/180937-7-3-27.wav']\r\n",
        "    # wavs = ['/content/drive/MyDrive/Training data/audio_train/100652-3-0-0.wav']\r\n",
        "    # print(wavs2feat(wavs).shape)\r\n",
        "    # wavfiles = ['../shared_train/audio_train/180937-7-3-27.wav','../shared_train/audio_train/180937-7-3-27.wav']\r\n",
        "    # X = wavs2feat(wavs)\r\n",
        "    # eval('test_task1/labels.csv', 'test_task1/est.csv', 1)\r\n",
        "    # editDistance(\"dog_bark-street_music-engine_idling\",\"siren-street_music-engine_idling\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IejlYi3kvd-I"
      },
      "source": [
        "#**TASK 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5k3HcGZvk5L"
      },
      "source": [
        "##Data Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRR3Ti8wvZV4"
      },
      "source": [
        "# root = '/content/drive/MyDrive/Project/MLSP Course Project/audio_train_1ch/'\r\n",
        "# labels = pd.read_csv('/content/drive/MyDrive/Project/MLSP Course Project/labels_train.csv')\r\n",
        "root = '/content/drive/MyDrive/Training data/audio_train_1ch/'\r\n",
        "df = pd.read_csv('/content/drive/MyDrive/Training data/labels_train.csv')\r\n",
        "filename = list(df['slice_file_name'])\r\n",
        "classes = list(df['class'])\r\n",
        "dic = {}    #contains the file name along with their labels\r\n",
        "for i in filename:\r\n",
        "  for val in classes:\r\n",
        "    dic[i] = val\r\n",
        "    classes.remove(val)\r\n",
        "    break\r\n",
        "enc = {'air_conditioner': 0, 'car_horn': 1, 'children_playing': 2, 'dog_bark': 3, 'drilling': 4, 'engine_idling': 5, 'gun_shot': 6, 'jackhammer': 7, 'siren': 8, 'street_music': 9}\r\n",
        "finwav = []\r\n",
        "finlab = []\r\n",
        "N = 500 #Number of samples\r\n",
        "for i in range(N):\r\n",
        "  n = np.random.randint(2,6)\r\n",
        "  files = []\r\n",
        "  prev = []\r\n",
        "  wav = []\r\n",
        "  labels = []\r\n",
        "  for i in range(n):\r\n",
        "    wav,label = random.choice(list(dic.items()))\r\n",
        "    if len(prev) == 0:\r\n",
        "      prev.append(label)\r\n",
        "    elif prev[0] == label:\r\n",
        "      while(prev[0] == label):\r\n",
        "        wav,label = random.choice(list(dic.items()))\r\n",
        "      prev = []\r\n",
        "      prev.append(label)  \r\n",
        "    else:\r\n",
        "      prev = []\r\n",
        "      prev.append(label)  \r\n",
        "    files.append(wav)\r\n",
        "    labels.append(enc[label])  \r\n",
        "  wavfile = []\r\n",
        "  for i in range(n):\r\n",
        "    wavfile.append(root+files[i])\r\n",
        "  finwav.append(wavfile)\r\n",
        "  finlab.append(labels)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgLU46zXLPmv",
        "outputId": "d824ad48-b9a0-47bf-aa2f-6d27dce8601c"
      },
      "source": [
        "#Creating the feature array\r\n",
        "X_data = []\r\n",
        "for i in range(len(finwav)):\r\n",
        "  sample = wavs2feat(finwav[i])\r\n",
        "  concan = np.zeros((513,2005-sample.shape[1]))\r\n",
        "  sample = np.concatenate((sample, concan), axis=1)\r\n",
        "  X_data.append(sample)\r\n",
        "  print(i)\r\n",
        "X_data = np.array(X_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TBGOJJMP1o2"
      },
      "source": [
        "#Creating the target array\r\n",
        "Y_data = []\r\n",
        "for i in range(len(finlab)):\r\n",
        "  temp = np.zeros((10,5))\r\n",
        "  # c = 0\r\n",
        "  for j in range(len(finlab[i])):\r\n",
        "    temp[finlab[i][j]][j] = 1\r\n",
        "  Y_data.append(temp.T)\r\n",
        "Y_data = np.array(Y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wefpl426S-tO"
      },
      "source": [
        "X_data = X_data.reshape(X_data.shape[0],X_data.shape[1],X_data.shape[2],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9BrmNZMN6Nh"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbyUbUH16Rf-"
      },
      "source": [
        "from keras.layers import Bidirectional, TimeDistributed, Conv2D, MaxPooling2D, Input, GRU, Dense, Activation, Dropout, Reshape, Permute\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.models import Model\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TdkdEwJ60Kk"
      },
      "source": [
        "def get_model(data_in, data_out, _cnn_nb_filt, _cnn_pool_size, _rnn_nb, _fc_nb):\r\n",
        "\r\n",
        "    #spec_start = Input(shape=(data_in.shape[-3], data_in.shape[-2], data_in.shape[-1]))\r\n",
        "    \r\n",
        "    spec_start = Input(shape=(data_in.shape[-3], data_in.shape[-2], data_in.shape[-1]))\r\n",
        "    spec_x = spec_start\r\n",
        "\r\n",
        "    for _i, _cnt in enumerate(_cnn_pool_size):\r\n",
        "        spec_x = Conv2D(filters=_cnn_nb_filt, kernel_size=(3, 3), padding='same')(spec_x)\r\n",
        "        spec_x = BatchNormalization(axis=1)(spec_x)\r\n",
        "        spec_x = Activation('relu')(spec_x)\r\n",
        "        spec_x = MaxPooling2D(pool_size=(1, _cnn_pool_size[_i]* _cnn_pool_size[_i]))(spec_x)\r\n",
        "        spec_x = Dropout(0.3)(spec_x)\r\n",
        "    spec_x = Permute((2, 1, 3))(spec_x)\r\n",
        "    spec_x = Reshape((spec_x.shape[-3], spec_x.shape[-2]*spec_x.shape[-1]))(spec_x)\r\n",
        "\r\n",
        "    for _r in _rnn_nb:\r\n",
        "        spec_x = Bidirectional(\r\n",
        "            GRU(_r, activation='tanh', dropout=0.3, recurrent_dropout=0.3, return_sequences=True),merge_mode='mul')(spec_x)\r\n",
        "\r\n",
        "    for _f in _fc_nb:\r\n",
        "        spec_x = TimeDistributed(Dense(_f))(spec_x)\r\n",
        "        spec_x = Dropout(0.3)(spec_x)\r\n",
        "\r\n",
        "    spec_x = TimeDistributed(Dense(data_out.shape[-1]))(spec_x)\r\n",
        "    out = Activation('softmax', name='strong_out')(spec_x)\r\n",
        "\r\n",
        "    model = Model(inputs=spec_start, outputs=out)\r\n",
        "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE3X4Z9nS14Z",
        "outputId": "dcbadd2d-0ea3-4d9e-86d3-c71887175fc7"
      },
      "source": [
        "model = get_model(X_data,Y_data,16,[5,2,2],[8,8],[8])\r\n",
        "history = model.fit(x=X_data, y=Y_data, epochs = 100, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 513, 2005, 1)]    0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 513, 2005, 16)     160       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 513, 2005, 16)     2052      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 513, 2005, 16)     0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 513, 80, 16)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 513, 80, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 513, 80, 16)       2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 513, 80, 16)       2052      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 513, 80, 16)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 513, 20, 16)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 513, 20, 16)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 513, 20, 16)       2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 513, 20, 16)       2052      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 513, 20, 16)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 513, 5, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 513, 5, 16)        0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 5, 513, 16)        0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 5, 8208)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 5, 8)              394464    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 5, 8)              864       \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 5, 8)              72        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 8)              0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 5, 10)             90        \n",
            "_________________________________________________________________\n",
            "strong_out (Activation)      (None, 5, 10)             0         \n",
            "=================================================================\n",
            "Total params: 406,446\n",
            "Trainable params: 403,368\n",
            "Non-trainable params: 3,078\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "16/16 [==============================] - 6s 357ms/step - loss: 1.5839 - accuracy: 0.0980\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.5768 - accuracy: 0.1000\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.5743 - accuracy: 0.1180\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.5676 - accuracy: 0.1384\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.5607 - accuracy: 0.1620\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.5557 - accuracy: 0.1812\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.5493 - accuracy: 0.1704\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.5459 - accuracy: 0.1908\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.5426 - accuracy: 0.2092\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.5414 - accuracy: 0.1764\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.5305 - accuracy: 0.1896\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.5249 - accuracy: 0.1720\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.5207 - accuracy: 0.1636\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.5082 - accuracy: 0.1648\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.5073 - accuracy: 0.1572\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.4972 - accuracy: 0.1484\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.4954 - accuracy: 0.1492\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.4783 - accuracy: 0.1540\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.4686 - accuracy: 0.1612\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.4614 - accuracy: 0.1612\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.4538 - accuracy: 0.1668\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.4417 - accuracy: 0.1776\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.4362 - accuracy: 0.1704\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.4327 - accuracy: 0.1668\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.4171 - accuracy: 0.1804\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.4096 - accuracy: 0.1884\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.4089 - accuracy: 0.1804\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.3935 - accuracy: 0.1808\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 1.4077 - accuracy: 0.1796\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 1.4021 - accuracy: 0.1932\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.3886 - accuracy: 0.2000\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.3764 - accuracy: 0.1972\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.3826 - accuracy: 0.2020\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.3483 - accuracy: 0.2060\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.3589 - accuracy: 0.1972\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.3367 - accuracy: 0.2072\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.3458 - accuracy: 0.2152\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.3242 - accuracy: 0.2148\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.3145 - accuracy: 0.2428\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.2963 - accuracy: 0.2304\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.3097 - accuracy: 0.2176\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.3193 - accuracy: 0.2228\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 5s 332ms/step - loss: 1.2985 - accuracy: 0.2280\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.3138 - accuracy: 0.2180\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.2835 - accuracy: 0.2300\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.2648 - accuracy: 0.2360\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.2737 - accuracy: 0.2300\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.2670 - accuracy: 0.2392\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 1.2593 - accuracy: 0.2344\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.2618 - accuracy: 0.2304\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.2526 - accuracy: 0.2520\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.2424 - accuracy: 0.2364\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.2305 - accuracy: 0.2388\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.2242 - accuracy: 0.2436\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.2272 - accuracy: 0.2536\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 1.2196 - accuracy: 0.2588\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 5s 330ms/step - loss: 1.2034 - accuracy: 0.2684\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.2210 - accuracy: 0.2632\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.1971 - accuracy: 0.2572\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.2065 - accuracy: 0.2712\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 5s 341ms/step - loss: 1.2046 - accuracy: 0.2708\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.1652 - accuracy: 0.2636\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.1452 - accuracy: 0.2840\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.1702 - accuracy: 0.2804\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.1839 - accuracy: 0.2724\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.1709 - accuracy: 0.2784\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.1471 - accuracy: 0.2948\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.1556 - accuracy: 0.3000\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 5s 332ms/step - loss: 1.1406 - accuracy: 0.2860\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.1353 - accuracy: 0.3008\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 5s 332ms/step - loss: 1.1391 - accuracy: 0.3080\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.1479 - accuracy: 0.2900\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.1401 - accuracy: 0.3012\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.1084 - accuracy: 0.3132\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.1162 - accuracy: 0.2956\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.1179 - accuracy: 0.3076\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 1.1210 - accuracy: 0.3196\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.1036 - accuracy: 0.3144\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 5s 339ms/step - loss: 1.0891 - accuracy: 0.3304\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.1028 - accuracy: 0.3364\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.0973 - accuracy: 0.3284\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 5s 328ms/step - loss: 1.0835 - accuracy: 0.3280\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 5s 340ms/step - loss: 1.1050 - accuracy: 0.3148\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0980 - accuracy: 0.3372\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0974 - accuracy: 0.3212\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.0817 - accuracy: 0.3364\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.0781 - accuracy: 0.3408\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0918 - accuracy: 0.3300\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0722 - accuracy: 0.3320\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.0341 - accuracy: 0.3432\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.0502 - accuracy: 0.3488\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 5s 337ms/step - loss: 1.0555 - accuracy: 0.3544\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 5s 336ms/step - loss: 1.0384 - accuracy: 0.3440\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0666 - accuracy: 0.3472\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.0797 - accuracy: 0.3392\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 5s 333ms/step - loss: 1.1259 - accuracy: 0.3232\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 5s 334ms/step - loss: 1.0679 - accuracy: 0.3268\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 5s 338ms/step - loss: 1.0753 - accuracy: 0.3284\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 5s 335ms/step - loss: 1.0334 - accuracy: 0.3444\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 5s 331ms/step - loss: 1.0331 - accuracy: 0.3708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjsL-dj7sMpl",
        "outputId": "288adb21-5d7b-40e3-a066-e003bca22b28"
      },
      "source": [
        "# model.save('/content/drive/MyDrive/Training data/My model1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Training data/My model1/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cga-xoUrjlx9"
      },
      "source": [
        "##Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twxp3570jlNF"
      },
      "source": [
        "# root1 = '/content/drive/MyDrive/Project/MLSP Course Project/sample_test_task1/feats/'\r\n",
        "# df1 = pd.read_csv('/content/drive/MyDrive/Project/MLSP Course Project/sample_test_task1/labels.csv')\r\n",
        "# root1 = '/content/drive/MyDrive/Training data/sample_test_task2/feats/'\r\n",
        "# df1 = pd.read_csv('/content/drive/MyDrive/Training data/sample_test_task2/labels.csv')\r\n",
        "# df1 = np.array(df1)\r\n",
        "# test_data = []\r\n",
        "# sample = np.load(root1 + 'a00001.npy') \r\n",
        "# concan = np.zeros((513,2005-sample.shape[1]))\r\n",
        "# sample = np.concatenate((sample, concan), axis=1)\r\n",
        "# test_data.append(sample)\r\n",
        "# i = 1\r\n",
        "# for file in df1[:,:1]:\r\n",
        "#   sample = np.load(root1 + file[0] + '.npy') \r\n",
        "#   concan = np.zeros((513,2005-sample.shape[1]))\r\n",
        "#   sample = np.concatenate((sample, concan), axis=1)\r\n",
        "#   test_data.append(sample)\r\n",
        "#   print(i)\r\n",
        "#   i = i+1\r\n",
        "\r\n",
        "test_data = []\r\n",
        "for i in range(1,31):\r\n",
        "  if (i<10):\r\n",
        "    path='/content/drive/MyDrive/Training data/test_task2/feats/a00'+str(i)+'.npy'\r\n",
        "  else:\r\n",
        "    path='/content/drive/MyDrive/Training data/test_task2/feats/a0'+str(i)+'.npy'\r\n",
        "  spect=np.load(path)\r\n",
        "  concan = np.zeros((513,2005-spect.shape[1]))\r\n",
        "  sample = np.concatenate((spect, concan), axis=1)\r\n",
        "  test_data.append(sample)\r\n",
        "test_data = np.array(test_data)\r\n",
        "X_test = test_data.reshape(test_data.shape[0], 513, 2005, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HqCO_LPspO8"
      },
      "source": [
        "pred = model.predict(X_test)\r\n",
        "pred = np.argmax(pred,axis=2)\r\n",
        "\r\n",
        "cl = list(enc.keys())\r\n",
        "la = list(enc.values())\r\n",
        " \r\n",
        "Y_pred = []         #list of predicted labels\r\n",
        "for i in range(len(pred)):\r\n",
        "  lab = ''\r\n",
        "  prev = []\r\n",
        "  for j in range(5):\r\n",
        "    position = la.index(pred[i][j])\r\n",
        "    if len(prev) == 0:\r\n",
        "      prev.append(cl[position])\r\n",
        "    elif prev[0] == cl[position]:\r\n",
        "      continue\r\n",
        "    else:\r\n",
        "      prev = []\r\n",
        "      prev.append(cl[position])    \r\n",
        "    lab += cl[position] + '-'\r\n",
        "  lab = lab[:-1]\r\n",
        "  Y_pred.append(lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatd8e8ostY-"
      },
      "source": [
        "##Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBEg5BcOsslc"
      },
      "source": [
        "fname = []\r\n",
        "for i in range(1,31):\r\n",
        "  if (i<10):\r\n",
        "    temp_str='a00'+str(i)+'.npy'\r\n",
        "  else:\r\n",
        "    temp_str='a00'+str(i)+'.npy'\r\n",
        "  fname.append(temp_str)\r\n",
        "dict = {'name': fname, 'label': Y_pred}\r\n",
        "df2 = pd.DataFrame(dict)\r\n",
        "df2.to_csv('/content/drive/MyDrive/Training data/task3_labels_test.csv', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ivGjyxjxsQD",
        "outputId": "e36b8d36-4543-45f6-f4a8-cd9f69f3b479"
      },
      "source": [
        "gtcsv = '/content/drive/MyDrive/Training data/task2_labels_test.csv'\r\n",
        "estcsv = '/content/drive/MyDrive/Training data/task3_labels_test.csv'\r\n",
        "evals(gtcsv,estcsv,2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}